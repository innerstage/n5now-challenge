{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca38718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from helper import spark_rename_column, pandas_rename_column\n",
    "from static import COLUMN_NAMES_DICT, COLUMN_TYPES_DICT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89313fd2",
   "metadata": {},
   "source": [
    "## Load source files to Spark and generate parquet files\n",
    "\n",
    "In this first step, we have several files in the `archive/` directory, we want to load these CSV files as Spark DataFrames and then transform them to parquet files inside the `spark_output/` folder.\n",
    "\n",
    "In order to achieve this, we have the following steps:\n",
    "\n",
    "* We build a SparkSession object (This is connected to my Dockerized environment: JupyterLab + 1 Master Node + 2 Worker Nodes)\n",
    "* We get a list of the files in the source folder programmatically, using a list comprehension with methods from the `os` library.\n",
    "* We also make sure the output directory exists before processing the files, using a simple `if` statement.\n",
    "* This might not be the tidiest way to do it, but for each source file, I defined a nested dictionary to write the column name and type changes we need to perform so Spark is able to write the parquet files. The dictionaries are in the `static.py` file. There are additional helping functions on the `helper.py` file as well.\n",
    "* Finally, we process each file by reading the CSV into a Spark DataFrame, change the column names/types and then generate the parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48468476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We build the Spark Session object\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"n5-challenge\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11f8792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the source files inside the folder\n",
    "filenames = sorted([f.replace('.csv','') for f in os.listdir('archive') if '.csv' in f])\n",
    "\n",
    "# We make sure the spark_output folder exists, so we can store the resulting parquets\n",
    "if not os.path.isdir('spark_output'):\n",
    "    os.mkdir('spark_output')\n",
    "    \n",
    "# We process each one of the source files\n",
    "for f in filenames:\n",
    "    # We read each CSV file into a Spark DataFrame\n",
    "    sdf = spark.read.csv('archive/' + f + '.csv', header=True)\n",
    "    \n",
    "    # We perform the column changes using the name and type dictionaries\n",
    "    names = COLUMN_NAMES_DICT[f]\n",
    "    for c in names.keys():\n",
    "        sdf = sdf.withColumnRenamed(c, names[c])\n",
    "        \n",
    "    for col_type in COLUMN_TYPES_DICT[f].keys():\n",
    "        for col_name in COLUMN_TYPES_DICT[f][col_type]:\n",
    "            sdf = spark_rename_column(sdf, col_name, col_type)\n",
    "        \n",
    "    # We generate the parquet file\n",
    "    sdf.write.mode('overwrite').parquet('spark_output/' + f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba9f897",
   "metadata": {},
   "source": [
    "## Spark-generated parquet files validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92231cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in filenames:\n",
    "    sdf = spark.read.parquet('spark_output/' + f)\n",
    "    sdf.show(5)\n",
    "    sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5274132f",
   "metadata": {},
   "source": [
    "## Load the source files into `pandas` and generate parquet files\n",
    "\n",
    "Following the same logic, we want to load these CSV files as pandas DataFrames and then transform them to parquet files inside the `pandas_output/` folder.\n",
    "\n",
    "In order to achieve this, we have the following steps:\n",
    "\n",
    "* We use the list of the files in the source folder that we got in the previous step.\n",
    "* We also make sure the output directory exists before processing the files, using a simple `if` statement.\n",
    "* Finally, we process each file by reading the CSV into a pandas DataFrame, change the column names/types and then generate the parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aebbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make sure the pandas_output folder exists, so we can store the resulting parquets\n",
    "if not os.path.isdir('pandas_output'):\n",
    "    os.mkdir('pandas_output')\n",
    "    \n",
    "# We process each one of the source files\n",
    "for f in filenames:\n",
    "    # We read each CSV file into a pandas DataFrame\n",
    "    pdf = pd.read_csv('archive/' + f + '.csv')\n",
    "    \n",
    "    # We perform the column changes using the name and type dictionaries\n",
    "    pdf = pdf.rename(columns=COLUMN_NAMES_DICT[f])\n",
    "    \n",
    "    for col_type in COLUMN_TYPES_DICT[f].keys():\n",
    "        for col_name in COLUMN_TYPES_DICT[f][col_type]:\n",
    "            pdf[col_name] = pandas_rename_column(pdf, col_name, col_type)\n",
    "        \n",
    "    # We generate the parquet file\n",
    "    pdf.to_parquet('pandas_output/' + f + '.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7db2a4",
   "metadata": {},
   "source": [
    "## pandas-generated parquet files validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41babfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in filenames:\n",
    "    pdf = pd.read_parquet('pandas_output/' + f + '.parquet')\n",
    "    print(pdf.head())\n",
    "    print(pdf.dtypes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
